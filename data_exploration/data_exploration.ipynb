{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Some initial investigation into seeing the connection between the ICF and Protocols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Handling imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/btor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # get punkt to use sentence tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "import torch\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/bterr/Projects/artos_icf_generation/.direnv/python-3.11.9/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/mnt/c/Users/bterr/Projects/artos_icf_generation/.direnv/python-3.11.9/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\") # we use this model for semantic similarity later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Data\n",
    "\n",
    "The `retrieve_trials.py` script downloads copies of clinical protocols and informed consent forms to the `icf/` and `prot/` directories. We first need to load this data for analysis.\n",
    "\n",
    "The amount of protocols may exceed the amount of available informed consent forms. We can track both the icf and prot forms by their associated ID, which ensures we can match them together later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icf_files = {i.name.split(\"_\")[0]: i for i in Path(\"icf\").glob(\"**/*\")}\n",
    "prot_files = {i.name.split(\"_\")[0]: i for i in Path(\"prot\").glob(\"**/*\")}\n",
    "len(icf_files), len(prot_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text from the PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "icf_text = {}\n",
    "prot_text = {}\n",
    "for k in icf_files.keys():\n",
    "    if k not in prot_files:\n",
    "        continue\n",
    "    reader = PdfReader(prot_files[k])\n",
    "    text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    prot_text[k] = text\n",
    "    \n",
    "    reader = PdfReader(icf_files[k])\n",
    "    text = \"\\n\\n\".join([page.extract_text() for page in reader.pages])\n",
    "    icf_text[k] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarity\n",
    "\n",
    "We can take a few approaches to calculating the overlap between the text:\n",
    "\n",
    "* Semantic Similarity - generate a text embedding for both documents, then calculate their similarity. This is likely ineffective, since the protocols are significantly larger than the ICFs, and may have details that are intentionally included in the ICFs\n",
    "* Ngram overlap - generate a few ngrams based off of the protocol and ICF files, then calculate based on the commonly shared ngrams. This is better, but still not ideal since the ngram calcluation I have does not respect sentences. Furthermore, minor paraphrased gaps between the text will not be accounted for by this approach. (You could pair it up with semantic similarity between ngrams, but at that point it may be more effort than its worth.)\n",
    "* fuzzy matching - between the two text calculate their levenshtein distance from one another. This is usually useful for detecting similarities between texts that have been modified off of each other. However, it isn't as effective here, since the protocol documents are just so much larger than the ICF documents.\n",
    "* Sentence Semantic Similarity - generate text embedding for all sentences in the documents, then calculate a similarity matrix from the sentences. Based off of the similarity matrix, find the highest similarity score for a given sentence in the ICF, and take the average count of those scores across the document\n",
    "\n",
    "The Sentence Semantic Similarity approach is the one I primarily pursued. We want to take the maximum sentence similarity, as it more strongly accounts for the hypothesis that the ICFs are informed by the protocols. Minimum and average similarity are unlikely to be useful since these scores will be artificially low. The protocols and ICFs cover a range of semanticly distinct topics (procedures, risks, boilerplate text), so it cannot be assumed that all sentences will share any degree of relevancy.\n",
    "\n",
    "For the sake of completness, I do offer code on how to explore the other methods as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text: str, N: int=2):\n",
    "    \"\"\"given a text, generate ngrams\"\"\"\n",
    "    return set([text[i: i + N] for i in range(len(text) - N +1)])\n",
    "\n",
    "def ngram_overlap(icf_text: str, prot_text: str, N=2):\n",
    "    \"\"\"given icf and prot, return number of shared ngrams that appear in the icf\"\"\"\n",
    "    icf_ngrams: set = generate_ngrams(icf_text, N)\n",
    "    prot_ngrams: set = generate_ngrams(prot_text, N)\n",
    "\n",
    "    num_shared_ngrams = len(icf_ngrams & prot_ngrams)\n",
    "    total_ngrams = len(icf_ngrams | prot_ngrams) # this may not be useful, since we just care about overlap from the ICF\n",
    "    return num_shared_ngrams / len(icf_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "\n",
    "def fuzzy_matching(icf_text: str, prot_text: str):\n",
    "    \"\"\"calculate levenshtein Distance between two text\"\"\"\n",
    "    return fuzz.ratio(icf_text, prot_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_icf_similarity_matrix(icf_text: str, prot_text: str, model):\n",
    "    \"\"\"Given two blocks of text, calculate their sentence similarity matrix\"\"\"\n",
    "    icf_sentences = sent_tokenize(icf_text)\n",
    "    prot_sentences = sent_tokenize(prot_text)\n",
    "    icf_embeddings = model.encode(icf_sentences)\n",
    "    prot_embeddings = model.encode(prot_sentences)\n",
    "\n",
    "    return model.similarity(icf_embeddings, prot_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:56<00:00,  5.83s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "scores = {}\n",
    "for key in tqdm(icf_text.keys()):\n",
    "\n",
    "    # account for places where we have the id for icf\n",
    "    #   but not for protocols\n",
    "    if key not in prot_text:\n",
    "        continue\n",
    "    \n",
    "    i = icf_text[key]\n",
    "    p = prot_text[key]\n",
    "\n",
    "    similarities = calculate_icf_similarity_matrix(i, p, model)\n",
    "    max_median = torch.max(similarities, axis=1)[0].median()\n",
    "    max_mean = torch.max(similarities, axis=1)[0].mean()\n",
    "\n",
    "    scores[key] = max_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Calculating scores we get an average overlap of around ~0.785 between the ICF onto the Protocols. This is high enough for me to infer that the text in the ICF is highly similar to the text in the Protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7851)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(scores.values())/len(scores) # calculate the average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
